<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="这篇里记录一下梯度下降在一般条件下的收敛分析。关键的思想有\n利用泰勒公式+拉格朗日余项对函数做二阶展开。 Smooth和Convexity分别对梯度、海塞引入上下界。 收敛$\\Leftrightarrow f$是Lipschitz smooth（stochastic情形下也需要假设次梯度方差有界，这等于是说smooth，甚至比smooth更强因为smoothness等价于梯度平方有上界）。 收敛速度取决于$|\\nabla f(x)|$的下界，即有多convex。 注：用$|x-x^\\star|^2$和$f(x)-f(x^\\star)$来推导结果是一致的，前者用完全平方公式展开，后者用泰勒展开。\n表格结果 假设Lipschitz平滑常数为$M$，强凸常数为$m$，初始距离最优值距离$|x_0-x^\\star|\\leq r$，初始函数值差距$f(x_0)-f(x^\\star)\\leq R$，随机情形下假设$\\mathbb{E}[|\\tilde g_{\\theta}|\\theta|^2]\\leq B^2$，有以下结果：\nMethods Non-smooth Smooth+Non-convex Smooth+Convex Smooth+Strong Convexity Gradient Descent May Divergent Converge to local optima $O(\\frac{Mr^2}{K})$ $O\\left(\\left(1-\\frac{m}{M}\\right)^KR\\right)$ Stochastic Gradient Descent May Divergent Almost surely converge to Critical points $O(\\frac{Br}{\\sqrt{K}})$ $O(\\frac{B^2}{mK})$ 对于一般非凸非光滑问题的收敛速度的界我们没有好的结果，因为这至少是NP难问题。\n基础 考虑可导函数$f:\\mathbb{R}^d\\rightarrow\\mathbb{R}$，在任一点处展开有：\n$$ f(x) = f(y) + (x-y)^T\\nabla f(y) + (x-y)^T\\nabla^2f(c)(x-y), \\tag{1} $$其中$c$是$x$和$y$线段上一点。\nConvexity、Strong Convexity、Lipschitz Smooth 我们说他是凸函数，意味着$\\forall c\\in\\mathbf{dom}f, \\nabla^2f(c)\\succeq0$，即$\\forall x, y\\in \\mathbf{dom}f$\n$$ f(x) \\geq f(y) + (x-y)^T\\nabla f(y). $$Lipschitz smooth和Strong convexity类似，是对$\\nabla^2f(c)$引入了上下界。假设$f(x)$是$M$-Lipschitz smooth，以及$m$-strongly convex，有$\\forall c\\in\\mathbf{dom}f$\n"><title>梯度下降收敛分析</title>
<link rel=canonical href=https://fingertap.github.io/p/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%94%B6%E6%95%9B%E5%88%86%E6%9E%90/><link rel=stylesheet href=/scss/style.min.663803bebe609202d5b39d848f2d7c2dc8b598a2d879efa079fa88893d29c49c.css><meta property='og:title' content="梯度下降收敛分析"><meta property='og:description' content="这篇里记录一下梯度下降在一般条件下的收敛分析。关键的思想有\n利用泰勒公式+拉格朗日余项对函数做二阶展开。 Smooth和Convexity分别对梯度、海塞引入上下界。 收敛$\\Leftrightarrow f$是Lipschitz smooth（stochastic情形下也需要假设次梯度方差有界，这等于是说smooth，甚至比smooth更强因为smoothness等价于梯度平方有上界）。 收敛速度取决于$|\\nabla f(x)|$的下界，即有多convex。 注：用$|x-x^\\star|^2$和$f(x)-f(x^\\star)$来推导结果是一致的，前者用完全平方公式展开，后者用泰勒展开。\n表格结果 假设Lipschitz平滑常数为$M$，强凸常数为$m$，初始距离最优值距离$|x_0-x^\\star|\\leq r$，初始函数值差距$f(x_0)-f(x^\\star)\\leq R$，随机情形下假设$\\mathbb{E}[|\\tilde g_{\\theta}|\\theta|^2]\\leq B^2$，有以下结果：\nMethods Non-smooth Smooth+Non-convex Smooth+Convex Smooth+Strong Convexity Gradient Descent May Divergent Converge to local optima $O(\\frac{Mr^2}{K})$ $O\\left(\\left(1-\\frac{m}{M}\\right)^KR\\right)$ Stochastic Gradient Descent May Divergent Almost surely converge to Critical points $O(\\frac{Br}{\\sqrt{K}})$ $O(\\frac{B^2}{mK})$ 对于一般非凸非光滑问题的收敛速度的界我们没有好的结果，因为这至少是NP难问题。\n基础 考虑可导函数$f:\\mathbb{R}^d\\rightarrow\\mathbb{R}$，在任一点处展开有：\n$$ f(x) = f(y) + (x-y)^T\\nabla f(y) + (x-y)^T\\nabla^2f(c)(x-y), \\tag{1} $$其中$c$是$x$和$y$线段上一点。\nConvexity、Strong Convexity、Lipschitz Smooth 我们说他是凸函数，意味着$\\forall c\\in\\mathbf{dom}f, \\nabla^2f(c)\\succeq0$，即$\\forall x, y\\in \\mathbf{dom}f$\n$$ f(x) \\geq f(y) + (x-y)^T\\nabla f(y). $$Lipschitz smooth和Strong convexity类似，是对$\\nabla^2f(c)$引入了上下界。假设$f(x)$是$M$-Lipschitz smooth，以及$m$-strongly convex，有$\\forall c\\in\\mathbf{dom}f$\n"><meta property='og:url' content='https://fingertap.github.io/p/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%94%B6%E6%95%9B%E5%88%86%E6%9E%90/'><meta property='og:site_name' content='张晗的随笔'><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='Machine Learning'><meta property='article:tag' content='Math'><meta property='article:published_time' content='2019-07-28T23:34:55+08:00'><meta property='article:modified_time' content='2019-07-28T23:34:55+08:00'><meta property='og:image' content='https://fingertap.github.io/p/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%94%B6%E6%95%9B%E5%88%86%E6%9E%90/cover.png'><meta name=twitter:title content="梯度下降收敛分析"><meta name=twitter:description content="这篇里记录一下梯度下降在一般条件下的收敛分析。关键的思想有\n利用泰勒公式+拉格朗日余项对函数做二阶展开。 Smooth和Convexity分别对梯度、海塞引入上下界。 收敛$\\Leftrightarrow f$是Lipschitz smooth（stochastic情形下也需要假设次梯度方差有界，这等于是说smooth，甚至比smooth更强因为smoothness等价于梯度平方有上界）。 收敛速度取决于$|\\nabla f(x)|$的下界，即有多convex。 注：用$|x-x^\\star|^2$和$f(x)-f(x^\\star)$来推导结果是一致的，前者用完全平方公式展开，后者用泰勒展开。\n表格结果 假设Lipschitz平滑常数为$M$，强凸常数为$m$，初始距离最优值距离$|x_0-x^\\star|\\leq r$，初始函数值差距$f(x_0)-f(x^\\star)\\leq R$，随机情形下假设$\\mathbb{E}[|\\tilde g_{\\theta}|\\theta|^2]\\leq B^2$，有以下结果：\nMethods Non-smooth Smooth+Non-convex Smooth+Convex Smooth+Strong Convexity Gradient Descent May Divergent Converge to local optima $O(\\frac{Mr^2}{K})$ $O\\left(\\left(1-\\frac{m}{M}\\right)^KR\\right)$ Stochastic Gradient Descent May Divergent Almost surely converge to Critical points $O(\\frac{Br}{\\sqrt{K}})$ $O(\\frac{B^2}{mK})$ 对于一般非凸非光滑问题的收敛速度的界我们没有好的结果，因为这至少是NP难问题。\n基础 考虑可导函数$f:\\mathbb{R}^d\\rightarrow\\mathbb{R}$，在任一点处展开有：\n$$ f(x) = f(y) + (x-y)^T\\nabla f(y) + (x-y)^T\\nabla^2f(c)(x-y), \\tag{1} $$其中$c$是$x$和$y$线段上一点。\nConvexity、Strong Convexity、Lipschitz Smooth 我们说他是凸函数，意味着$\\forall c\\in\\mathbf{dom}f, \\nabla^2f(c)\\succeq0$，即$\\forall x, y\\in \\mathbf{dom}f$\n$$ f(x) \\geq f(y) + (x-y)^T\\nabla f(y). $$Lipschitz smooth和Strong convexity类似，是对$\\nabla^2f(c)$引入了上下界。假设$f(x)$是$M$-Lipschitz smooth，以及$m$-strongly convex，有$\\forall c\\in\\mathbf{dom}f$\n"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content='https://fingertap.github.io/p/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%94%B6%E6%95%9B%E5%88%86%E6%9E%90/cover.png'><link rel="shortcut icon" href=/favicon.png></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label=切换菜单>
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hu_ce70c8f8535541a2.png width=300 height=300 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>🇨🇳</span></figure><div class=site-meta><h1 class=site-name><a href=/>张晗的随笔</a></h1><h2 class=site-description>看风景 > 爬上山顶</h2></div></header><ol class=menu id=main-menu><li><a href=/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li><a href=/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li><a href=/about-me/><svg class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg>
<span>About Me</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>暗色模式</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">目录</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#表格结果>表格结果</a></li><li><a href=#基础>基础</a><ol><li><a href=#convexitystrong-convexitylipschitz-smooth>Convexity、Strong Convexity、Lipschitz Smooth</a></li><li><a href=#machine-learning-lossgradient-descentstochastic-gradient-descent>Machine Learning Loss、Gradient Descent、Stochastic Gradient Descent</a></li><li><a href=#relationship-between-gradient-and-stochastic-gradientsubgradient>Relationship between Gradient and Stochastic Gradient、Subgradient</a></li></ol></li><li><a href=#收敛性分析>收敛性分析</a><ol><li><a href=#gradient-descent>Gradient Descent</a><ol><li><a href=#convex情形>Convex情形</a></li><li><a href=#m-strongly-convex情形>$m$-strongly convex情形</a></li></ol></li><li><a href=#stochastic-gradient-descent>Stochastic Gradient Descent</a><ol><li><a href=#convex情形-1>Convex情形</a></li><li><a href=#strong-convex情形>Strong Convex情形</a></li></ol></li></ol></li></ol></nav></div></section></aside><main class="main full-width"><article class="has-image main-article"><header class=article-header><div class=article-image><a href=/p/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%94%B6%E6%95%9B%E5%88%86%E6%9E%90/><img src=/p/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%94%B6%E6%95%9B%E5%88%86%E6%9E%90/cover_hu_969f1c0c6e2718b5.png srcset="/p/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%94%B6%E6%95%9B%E5%88%86%E6%9E%90/cover_hu_969f1c0c6e2718b5.png 800w, /p/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%94%B6%E6%95%9B%E5%88%86%E6%9E%90/cover_hu_e42ad02c82476eb8.png 1600w" width=800 height=800 loading=lazy alt="Featured image of post 梯度下降收敛分析"></a></div><div class=article-details><header class=article-category><a href=/categories/machine-learning/>Machine Learning</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%94%B6%E6%95%9B%E5%88%86%E6%9E%90/>梯度下降收敛分析</a></h2></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>Jul 28, 2019</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>阅读时长: 3 分钟</time></div></footer></div></header><section class=article-content><p>这篇里记录一下梯度下降在一般条件下的收敛分析。关键的思想有</p><ol><li>利用泰勒公式+拉格朗日余项对函数做二阶展开。</li><li>Smooth和Convexity分别对梯度、海塞引入上下界。</li><li>收敛$\Leftrightarrow f$是Lipschitz smooth（stochastic情形下也需要假设次梯度方差有界，这等于是说smooth，甚至比smooth更强因为smoothness等价于梯度平方有上界）。</li><li>收敛速度取决于$|\nabla f(x)|$的下界，即有多convex。</li></ol><p>注：用$|x-x^\star|^2$和$f(x)-f(x^\star)$来推导结果是一致的，前者用完全平方公式展开，后者用泰勒展开。</p><hr><h2 id=表格结果>表格结果</h2><p>假设Lipschitz平滑常数为$M$，强凸常数为$m$，初始距离最优值距离$|x_0-x^\star|\leq r$，初始函数值差距$f(x_0)-f(x^\star)\leq R$，随机情形下假设$\mathbb{E}[|\tilde g_{\theta}|\theta|^2]\leq B^2$，有以下结果：</p><div class=table-wrapper><table><thead><tr><th style=text-align:center>Methods</th><th style=text-align:center>Non-smooth</th><th style=text-align:center>Smooth+Non-convex</th><th style=text-align:center>Smooth+Convex</th><th style=text-align:center>Smooth+Strong Convexity</th></tr></thead><tbody><tr><td style=text-align:center>Gradient Descent</td><td style=text-align:center>May Divergent</td><td style=text-align:center>Converge to local optima</td><td style=text-align:center>$O(\frac{Mr^2}{K})$</td><td style=text-align:center>$O\left(\left(1-\frac{m}{M}\right)^KR\right)$</td></tr><tr><td style=text-align:center>Stochastic Gradient Descent</td><td style=text-align:center>May Divergent</td><td style=text-align:center>Almost surely converge to Critical points</td><td style=text-align:center>$O(\frac{Br}{\sqrt{K}})$</td><td style=text-align:center>$O(\frac{B^2}{mK})$</td></tr></tbody></table></div><p>对于一般非凸非光滑问题的收敛速度的界我们没有好的结果，因为这至少是NP难问题。</p><hr><h2 id=基础>基础</h2><p>考虑可导函数$f:\mathbb{R}^d\rightarrow\mathbb{R}$，在任一点处展开有：</p>$$
f(x) = f(y) + (x-y)^T\nabla f(y) + (x-y)^T\nabla^2f(c)(x-y),
\tag{1}
$$<p>其中$c$是$x$和$y$线段上一点。</p><h3 id=convexitystrong-convexitylipschitz-smooth>Convexity、Strong Convexity、Lipschitz Smooth</h3><p>我们说他是<strong>凸函数</strong>，意味着$\forall c\in\mathbf{dom}f, \nabla^2f(c)\succeq0$，即$\forall x, y\in \mathbf{dom}f$</p>$$
f(x) \geq f(y) + (x-y)^T\nabla f(y).
$$<p>Lipschitz smooth和Strong convexity类似，是对$\nabla^2f(c)$引入了上下界。假设$f(x)$是$M$-Lipschitz smooth，以及$m$-strongly convex，有$\forall c\in\mathbf{dom}f$</p>$$
mI\preceq \nabla^2f(c)\preceq MI,
$$<p>其中$I$是单位矩阵。注意这样一来Convex可以看做是0-strongly convex。Lipschitz smooth直觉理解就是没有折点（例如$|x|$在$x=0$处），Strong convexity直觉理解就是没有盆地（一片区域的函数值相等）。如果我们考虑最优值$x^\star$，对于强凸和smooth我们分别有</p>$$
2m\left\{f(x)-f(y)\right\}\leq\left\|\nabla f(x)\right\|^2\leq 2m\left\{f(x)-f(y)\right\}
$$<p>（代入式(1)，右边对$y$求极值）</p><h3 id=machine-learning-lossgradient-descentstochastic-gradient-descent>Machine Learning Loss、Gradient Descent、Stochastic Gradient Descent</h3><p>对于一族有监督统计学习模型$M_\theta$（模型的参数为$\theta\in\Theta$），设输入样本-标签对$(x, y)\in\mathcal{D}$满足$x\in\mathcal{X}, y\in\mathcal{Y}$，模型的决策函数（泛函）为$f_\theta:\mathcal{X}\rightarrow\mathcal{Y}$为连续映射。给定连续的损失函数$l:\mathcal{Y}\times\mathcal{Y}\rightarrow\mathbb{R}$，以及定义在$\mathcal{D}$上的概率度量$P$（密度函数为p），一个模型的好坏由期望损失给出：</p>$$
L(M_\theta)\triangleq \mathbb{E}_{(x, y)\in\mathcal{D}}[l(f_\theta(x), y)]
$$<p>如果我们拿不到输入空间$\mathcal{D}$（需要掌握所有可能的数据生成的方式，但是我们如果有这个生成方式还训练什么模型呢？直接查表不好吗？）和概率度量$P$，则这个期望只能由我们已有的数据集$D\subset\mathcal{D}$来近似，此时在$D$上的损失叫做经验损失：</p>$$
\hat{L}(M_\theta)\triangleq \frac{1}{|D|}\sum_{(x, y)\in D}l(f_\theta(x), y),
$$<p>这里假设了每个样本都是服从$P$，从$\mathcal{D}$中独立抽样出来的。则梯度下降\(GD\)的更新策略：</p>$$
\begin{align}
\theta_{k+1} &= \theta_k - \alpha \nabla l (\theta_k)\\
&=\theta_k - \alpha\cdot\frac{1}{D}\sum_{(x, y)\in D}\frac{\partial l(f_{\theta_k}(x), y)}{\partial \theta_k}.
\end{align}
$$<p>当$|D|$很大时计算$\nabla l(\theta_k)$的开支较大，随机梯度下降（SGD）对$D$进行采样，然后用带有随机性的梯度代替梯度下降中的$\nabla l(\theta_k)$。记采样得到的mini-batch为$D_k\subset D$，随机梯度定义为</p>$$
\nabla l_{D_k}(\theta_k) = \frac{1}{|D_k|}\sum_{(x, y)\in D_k}\frac{\partial l(f_{\theta_k}(x), y)}{\partial \theta_k}.
$$<p>由于任意一个样本$(x, y)$的损失的期望都是$L(M_\theta)$，可以很简单地证明随机梯度和经验梯度的期望都是期望损失在$\theta$处的梯度。</p><p>有时候随机梯度下降的随机性并不只是来自于对数据的采样（比如为了使目标变平滑，对数据加入随机白噪声），此时有可能使得随机梯度并不落在可行区域内（比如$\theta_k=[0.1, 0.9]$，而梯度为$-0.2, 1.1$，而我们希望$|\theta|<em>\infty\leq 1$），这时需要做一步正交投影操作，将更新后的$\theta</em>{k+1}$投影到可行区域$\Theta$内，方式是用最小二乘法在$\Theta$找一个距离$\theta_{k+1}$最近的点$\tilde\theta_{k+1}$，它满足$\forall \theta\in \Theta, |\theta-\theta_{k+1}|\geq|\theta-\tilde\theta_{k+1}|$，即投影后距离会缩小。</p><h3 id=relationship-between-gradient-and-stochastic-gradientsubgradient>Relationship between Gradient and Stochastic Gradient、Subgradient</h3><p>$f$在$x$处的次梯度（Subgradient）$g_x\in\mathbb{R}^d$是所有满足一阶条件的向量：$\forall x, y\in \mathbb{R}^d,$</p>$$
f(y)\geq f(x) + g_x^T(y-x),
$$<p>所有次梯度的集合叫做Subdifferential，记作$\partial f(x)$。当$f$在$x$处可导时，$\partial f(x) = {\nabla f(x)}$。也即，如果函数是光滑的，就不用考虑次梯度，对于非光滑的问题一般会用次梯度下降来分析收敛性等。</p><p>我们称一个向量$\tilde g_x$为$f$在$x$处的带噪无偏次梯度（Noisy Unbiased Subgradient）若$\mathbb{E}[\tilde g_x]=g_x\in\partial f(x)$。<strong>则随机梯度下降可以看做在梯度</strong>$\nabla l(\theta_k)$<strong>中引入一个零均值的加性噪声，而这个噪声为此次mini-batch的泛函，记作</strong>$v(D_k)$。</p><hr><h2 id=收敛性分析>收敛性分析</h2><h3 id=gradient-descent>Gradient Descent</h3><p><strong>结论</strong>：<em>梯度下降在无Smooth假设时可能不收敛，有M-smooth假设时收敛。达到精度</em>$\epsilon>0$<em>，凸时收敛速度为</em>$o(\frac{MR^2}{\epsilon})$<em>，强凸时收敛速度为</em>$o(\log_{1-m/M}\frac{\epsilon}{f(x_0)-f(x^\star)})$。</p><p>对$l(\theta)$没有任何假设的情况下，设$\theta^\star$使$l(\theta)$取得最小值，有：</p>$$
l(\theta_{k+1})-l(\theta^\star)
=\left\{l(\theta_k)-\alpha\nabla l(\theta_k)^T\nabla l(\theta_k)+\frac{\alpha^2}{2}\nabla l(\theta_k)^T\nabla^2 l(c)\nabla l(\theta_k)\right\}-l(\theta^\star),
$$<p>这里$c$是$\theta_k$和$\theta_{k+1}$线段上一点。如果$\nabla^2l(c)\rightarrow\infty$，则$\forall \alpha > 0, l(\theta_{k+1})-l(\theta^\star) >l(\theta_k)-l(\theta^\star)$，因此第$k$步迭代并没有降低损失。我们可以构造一个函数，使得从某个起点$\theta_0$开始，每一步梯度下降都是发散的。因此我们需要限制$\nabla^2l(\theta)\preceq MI$，即$l(\theta)$是$M$-Lipschitz smooth的。代入smooth条件有</p>$$
l(\theta_{k+1})-l(\theta^\star)
\leq\left\{\frac{\alpha^2M}{2}-\alpha\right\}\|\nabla l(\theta_k)\|^2+l(\theta_k)-l(\theta^\star),
$$<p>等式右侧对$\alpha$求最小，得$\alpha=1/M$时</p>$$
l(\theta_{k+1})-l(\theta^\star)
\leq -\frac{1}{2M}\left\|\nabla l(\theta_k)\right\|^2+l(\theta_k)-l(\theta^\star).
$$<p>因此当$0 &lt; \alpha &lt; 2/M$时，我们都有$l(\theta_{k+1}) - l(\theta^\star) &lt; l(\theta_k) - l(\theta^\star)$。</p><p><strong>要给出收敛速度需要对</strong>$|\nabla l(\theta_k)|$<strong>给出下界</strong>。</p><h4 id=convex情形>Convex情形</h4>$$
\begin{align}
&l(\theta^\star)\geq l(\theta_k)+\nabla l(\theta_k)^T(\theta^\star-\theta_k)\\
\Rightarrow~~& l(\theta_k)-l(\theta^\star)\leq\left\|\nabla l(\theta_k)^T(\theta^\star-\theta_k)\right\|\\
&~~~~~~~~~~~~~~~~~~~~~\leq\left\|\nabla l(\theta_k)\right\|\left\|\theta_k-\theta^\star\right\|\\
&~~~~~~~~~~~~~~~~~~~~~\leq\left\|\nabla l(\theta_k)\right\|\left\|\theta_0-\theta^\star\right\|\\
\Leftrightarrow~~&\|\nabla l(\theta_k)\|\geq\frac{l(\theta_k)-l(\theta^\star)}{\left\|\theta_0-\theta^\star\right\|}
\end{align}
$$<p>记$\eta_k=l(\theta_k)-l(\theta^\star)$，并假设$|\theta_0-\theta^\star|\leq R$，有</p>$$
\begin{align}
\eta_{k+1} &\leq \eta_k - \frac{1}{2M}\|\nabla l(\theta_k)\|^2\\
&\leq\eta_k-\frac{\eta_k^2}{2MR^2}
\end{align}
$$<p>化简方式是两边除以$\eta_k\eta_{k+1}$，整理得</p>$$
\begin{align}
&\frac{1}{\eta_{k+1}}-\frac{1}{\eta_k}\geq\frac{1}{2MR^2}\frac{\eta_k}{\eta_{k+1}}\geq\frac{1}{2MR^2}\\
\Rightarrow~~&\sum_{i=0}^k\frac{1}{\eta_{i+1}}-\frac{1}{\eta_i}=\frac{1}{\eta_{k+1}}-\frac{1}{\eta_0}\geq\frac{k+1}{2MR^2},
\end{align}
$$<p>即</p>$$
l(\theta_k)-l(\theta^\star)=\eta_k\leq\frac{2MR^2}{k},
$$<p>因此收敛速度是$O(1/k)$级别的次线性收敛。</p><h4 id=m-strongly-convex情形>$m$-strongly convex情形</h4><p>根据强凸定义（参考Lipschitz smooth的几种定义相互的推导）：</p>$$
\|\nabla l(\theta_k)\|^2 \geq 2m\left(l(\theta_k)-l(\theta^\star)\right)
$$<p>代入下界有</p>$$
l(\theta_k)-l(\theta^\star)=\eta_k\leq\left(1-\frac{m}{M}\right)^k\eta_0
$$<h3 id=stochastic-gradient-descent>Stochastic Gradient Descent</h3><p>随机情形下，假设$l(\theta)$为凸，记$\theta$处的带噪次梯度为$\tilde g_{\theta}$，并且$\exists B>0, \forall \theta\in\Theta, \mathbb{E}[\left|\tilde g_\theta\right|^2|\theta]\leq B^2$，且假设参数空间有界，即$\forall \theta \in \Theta, |\theta|\leq r$。考虑一般的随机梯度下降，即更新后可能落在可行域外，通过投影得到新的参数估计：</p>$$
\theta_{k+1} = Proj_\Theta(\theta_k-\alpha \tilde g_{\theta_k})，
$$<p>因为投影后的向量距离$\Theta$中的任意向量更近，有</p>$$
\begin{align}
\|\theta_{k+1}-\theta^\star\|^2 &\leq \|\theta_k-\alpha\tilde g_{\theta_k} -\theta^\star\|^2 \\
&=\|\theta_k-\theta^\star\|^2 + \alpha^2\|\tilde g_{\theta_k}\|^2-2\alpha\tilde g_{\theta_k}^T(\theta_k-\theta^\star)
\end{align}
$$<p>注意到这个式子中有两个随机变量$\theta_k$和$\tilde g_{\theta_k}$，后者依赖前者。因此我们这里对$\tilde g_{\theta_k}$在给定$\theta_k$的情况下求期望：</p>$$
\begin{align}
\mathbb{E}_{\tilde g_{\theta_k}}[\|\theta_{k+1}-\theta^\star\|^2|\theta_k] &\leq \|\theta_k-\theta^\star\|^2+ \alpha^2\mathbb{E}[\|\tilde g_{\theta_k}\|^2|\theta_k] - 2\alpha (\theta_k-\theta^\star)^T g_{\theta_k}\\
&\leq \|\theta_k-\theta^\star\|^2+ \alpha^2B^2 - 2\alpha (\theta_k-\theta^\star)^T g_{\theta_k}\\
\end{align}
$$<h4 id=convex情形-1>Convex情形</h4><p>利用函数的凸性，有</p>$$
\mathbb{E}_{\tilde g_{\theta_k}}[\|\theta_{k+1}-\theta^\star\|^2|\theta_k] \leq \|\theta_k-\theta^\star\|^2+ \alpha^2B^2 - 2\alpha \{l(\theta_k)-l(\theta^\star)\}
$$<p>再对$\theta_k$求期望，则之后所有的期望都是对$\tilde g_{\theta_k}$和$\theta_k$求的联合期望，因此下标之后省略。记$\gamma_k = \mathbb{E}[|\theta_{k}-\theta^\star|^2]$，整理有</p>$$
\begin{align}
& \mathbb{E}[l(\theta_k)] - l(\theta^\star) \leq \frac{1}{2\alpha} (\gamma_k-\gamma_{k+1}) + \frac{\alpha B^2}{2}\\
\Rightarrow~~& \sum_{i=0}^k\left\{\mathbb{E}[l(\theta_k)]-l(\theta^\star)\right\} \leq \frac{1}{2\alpha}(\gamma_0-\gamma_{k+1}) + \frac{(k+1)\alpha B^2}{2}\\
&~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\leq \frac{r^2}{2\alpha} + \frac{(k+1)\alpha B^2}{2}
\end{align}
$$<p>利用$\min$函数的凹性，并记$k$次迭代中最好的参数为$\theta_{best}$</p>$$
\begin{align}
\sum_{i=0}^k\left\{\mathbb{E}[l(\theta_k)]-l(\theta^\star)\right\} &\geq (k+1)\left\{\min_{i=0,\dots,k}\mathbb{E}[l(\theta_i)]-l(\theta^\star)\right\} \\
&\geq (k+1)\left\{\mathbb{E}[\min_{i=0,\dots,k}l(\theta_i)] - l(\theta^\star)\right\} \\
& = (k+1)\left\{\mathbb{E}[l(\theta_{best})]-l(\theta^\star)\right\}
\end{align}
$$<p>最后不等式右边对$\alpha$求最小，整理得</p>$$
\mathbb{E}[l(\theta_{best})] - l(\theta^\star) \leq \frac{Br}{\sqrt{k}}
$$<h4 id=strong-convex情形>Strong Convex情形</h4><p>强凸情形下用常数步长$\alpha$先把$k$个式子加起来再取最优的$\alpha^\star$并不能达到最优的收敛界。为了使得上界更紧，我们允许步长可变，即每一步有一个步长$\alpha_k$，然后对每个式子都取一个精心构造的步长，最后达到$O(1/K)$的收敛。利用强凸有</p>$$
\mathbb{E}_{\tilde g_{\theta_k}}[\|\theta_{k+1}-\theta^\star\|^2]\leq(1-\alpha_k m)\|\theta_k-\theta^\star\|^2 + \alpha_k^2B^2-2\alpha\{l(\theta_k)-l(\theta^\star)\}
$$<p>我们这里不能对$\alpha_k$直接取最优，因为最优值依赖于$|\theta_k-\theta^\star|^2$，而我们不知道$\theta^\star$的具体值。这里<strong>非常精妙</strong>地构造了一个步长$\alpha_k=1/km$，我还没弄懂怎么想到这样取的。Anyway，代入步长，对$\theta_k$求期望并记$\gamma_k=\mathbb{E}[|\theta_k-\theta^\star|^2],\eta_k=\mathbb{E}[l(\theta_k)-l(\theta^\star)]$有</p>$$
\begin{align}
\eta_k \leq \frac{B^2}{2km} + \frac{(k-1)m\gamma_k}{2} - \frac{km\gamma_{k+1}}{2}
\end{align}
$$<p>注意到右边最后两项可以被telescope消掉。两边乘以$k$并取telescope sum有</p>$$
\begin{align}
&k\cdot\eta_k \leq \frac{B^2}{2m} + \frac{k(k-1)m\gamma_k}{2}-\frac{k^2m\gamma_{k+1}}{2}\\
&~~~~~~~~~\leq\frac{B^2}{2m}+\frac{k(k-1)m\gamma_k}{2}-\frac{(k+1)km\gamma_{k+1}}{2}\\
\Rightarrow~~&\sum_{i=1}^{k}i\cdot\eta_i \leq \frac{B^2k}{2m}+ 0 -\frac{(k+1)km\gamma_{k+1}}{2}
\end{align}
$$<p>再利用之前的技巧$\eta_i\geq\min_{j=1,\dots,k}\mathbb{E}[l(\theta_j)]-l(\theta^\star)\geq\mathbb{E}[l(\theta_{best})]-l(\theta^\star)$，有</p>$$
\begin{align}
&\frac{(k+1)k}{2}\cdot\left\{\mathbb{E}[l(\theta_{best})]-l(\theta^\star)\right\} \leq \frac{B^2k}{2m}-\frac{(k+1)km\gamma_{k+1}}{2}\\
\Leftrightarrow~~&\mathbb{E}[l(\theta_{best})]-l(\theta^\star)\leq \frac{B^2}{mk}
\end{align}
$$</section><footer class=article-footer><section class=article-tags><a href=/tags/machine-learning/>Machine Learning</a>
<a href=/tags/math/>Math</a></section><section class=article-copyright><svg class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{const e=document.querySelector(".main-article");renderMathInElement(e,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],ignoredClasses:["gist"]})})</script></article><aside class=related-content--wrapper><h2 class=section-title>相关文章</h2><div class=related-content><div class="flex article-list--tile"><article class=has-image><a href=/p/%E4%B8%80%E7%A7%8D%E6%96%B0%E7%9A%84kl%E6%95%A3%E5%BA%A6%E4%BC%B0%E8%AE%A1/><div class=article-image><img src=/p/%E4%B8%80%E7%A7%8D%E6%96%B0%E7%9A%84kl%E6%95%A3%E5%BA%A6%E4%BC%B0%E8%AE%A1/cover.b4fff78720d50821465ba154fdc2df26_hu_3b4a1b04b09dd2a6.png width=250 height=150 loading=lazy alt="Featured image of post 一种新的KL散度估计" data-hash="md5-tP/3hyDVCCFGW6FU/cLfJg=="></div><div class=article-details><h2 class=article-title>一种新的KL散度估计</h2></div></a></article><article class=has-image><a href=/p/%E9%9A%90%E5%8F%98%E9%87%8F%E6%A8%A1%E5%9E%8B%E6%80%BB%E7%BB%93/><div class=article-image><img src=/p/%E9%9A%90%E5%8F%98%E9%87%8F%E6%A8%A1%E5%9E%8B%E6%80%BB%E7%BB%93/cover.14e58351554cfa73826426d3e2e1aace_hu_a47d951a9fa767ee.png width=250 height=150 loading=lazy alt="Featured image of post 隐变量模型总结" data-hash="md5-FOWDUVVM+nOCZCbT4uGqzg=="></div><div class=article-details><h2 class=article-title>隐变量模型总结</h2></div></a></article><article class=has-image><a href=/p/%E4%BB%8E%E6%8B%92%E7%BB%9D%E9%87%87%E6%A0%B7%E5%88%B0%E6%8A%95%E6%9C%BA%E6%8E%A8%E7%90%86/><div class=article-image><img src=/p/%E4%BB%8E%E6%8B%92%E7%BB%9D%E9%87%87%E6%A0%B7%E5%88%B0%E6%8A%95%E6%9C%BA%E6%8E%A8%E7%90%86/cover.84d3b9acd81cdce809bf40d9c86bf4b0_hu_2ec8dbcdde98afbe.png width=250 height=150 loading=lazy alt="Featured image of post 从拒绝采样到投机推理" data-hash="md5-hNO5rNgc3OgJv0DZyGv0sA=="></div><div class=article-details><h2 class=article-title>从拒绝采样到投机推理</h2></div></a></article></div></div></aside><footer class=site-footer><section class=copyright>&copy;
2025 张晗的随笔</section><section class=powerby>使用 <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> 构建<br>主题 <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.30.0>Stack</a></b> 由 <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a> 设计</section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>